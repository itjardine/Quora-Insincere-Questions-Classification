{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "6ae3b6b3897773cdf68f8f3f3ee55d9386c7b25d"
   },
   "source": [
    "# Insincere Question Classification\n",
    "## Quora Insincere Questions Classification Kaggle Challenge\n",
    "\n",
    "The purpose of this project is to study questions from Quora and to classify if the questions are sincere or not. \n",
    "\n",
    "To study this problem, we will work through studying this with a hybrid network of convolutional and LSTM units. Note this data can be obtained from the [Kaggle challenge](https://www.kaggle.com/c/quora-insincere-questions-classification)\n",
    "\n",
    "We begin by importing a number of packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "9171082f85c9656c8e2a3ecc5ff472e27700d001"
   },
   "outputs": [],
   "source": [
    "# General packages for data-analysis\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import re\n",
    "\n",
    "# Functions needed for tokenizing our comments\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Function to split data randomly\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Various layers for our eventual convolutions neural network (CNN)\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.layers import Conv1D\n",
    "from tensorflow.keras.layers import MaxPooling1D\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras import Sequential\n",
    "\n",
    "# For the F1 score, which is how the competition is judged\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# In order to compute the descision threshold to maximize the f1 score\n",
    "from scipy.optimize import minimize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "eb8d3b18314d7497426d7c860a072c260cd8d30f"
   },
   "source": [
    "## Data analysis\n",
    "\n",
    "We would like to knowmore about the data we are about to analyze. To do this, we can load it into a Pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "b30b9c2ecc85787e18685190b2b8309037d587dd"
   },
   "outputs": [],
   "source": [
    "# Loading our data into dataframes\n",
    "trainingdf = pd.read_csv('../input/train.csv')\n",
    "unknowndf = pd.read_csv('../input/test.csv')\n",
    "# We will take a look at the training data \n",
    "trainingdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "b5786e524c2ce96b4bba4b38f4be569b6ba418de"
   },
   "source": [
    "Now we take a look at the distribution of sincere or insincere labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "facd3722320231d51d27006bf6dd3f6e462ac479"
   },
   "outputs": [],
   "source": [
    "insincereexmaplesdf = trainingdf[trainingdf['target']==1]\n",
    "sincereexmaplesdf = trainingdf[trainingdf['target']==0]\n",
    "\n",
    "print(\"Number of insincere questions: \"+str(len(insincereexmaplesdf)))\n",
    "print(\"Number of sincere questions: \"+str(len(sincereexmaplesdf)))\n",
    "print(\"Percentage of insincere questions: \"+str(len(insincereexmaplesdf)/float(len(trainingdf))*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "70014777ba7fa7f85bea5faa674122e2a19b5f4d"
   },
   "source": [
    "So we can see that this is an imbalenced classification problem. We will likely need to deal with this properly later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "89e80325e39f5d6223cbdd142024e87ba56e93cf"
   },
   "outputs": [],
   "source": [
    "# For debugging/optimizing purposes, we will cut down on the amount of data to feed the neural network.\n",
    "#trainingdf = trainingdf[:20000]\n",
    "#insincereexmaplesdf = trainingdf[trainingdf['target']==1]\n",
    "#sincereexmaplesdf = trainingdf[trainingdf['target']==0]\n",
    "\n",
    "#print(\"Number of insincere questions: \"+str(len(insincereexmaplesdf)))\n",
    "#print(\"Number of sincere questions: \"+str(len(sincereexmaplesdf)))\n",
    "#print(\"Percentage of insincere questions: \"+str(len(insincereexmaplesdf)/float(len(trainingdf))*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "94850927e41447775744fd29998c083e671b187a"
   },
   "source": [
    "## Cleaning and tokenizing\n",
    "\n",
    "With some data analysis out of the way, we can look at cleaning and tokenizing the comments. We first want to replace contractions that are not in most of the embeddings later. \n",
    "\n",
    "We use the tokenizer first split up the comments into tokens (words) then assign each token a numeral integer. After some analysis of the tokenizing, we can then split the data in test and training sets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "1572f66c19a69a011eae8ffda72cb98a8b9af712"
   },
   "source": [
    "To start, we do some cleaning. There are two ways to do this, one is more accurate than the other but the more accurate one is meassured in minutes on the data compared to seconds for the less accurate one. The shorter, less accurate one is commented out for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "fa9ff42f4c6c180817dd2206640e96571270c426",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#%%time\n",
    "# Dictionaries of various corrections\n",
    "#contraction_mapping = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\", \"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\", \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",  \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\", \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\", \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\", \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\", \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\", \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\", \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\", \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\", \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\", \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\", \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",  \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\", \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\", \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\", \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\"}\n",
    "#punct_mapping = {\"‘\": \"'\", \"₹\": \"e\", \"´\": \"'\", \"°\": \"\", \"€\": \"e\", \"™\": \"tm\", \"√\": \" sqrt \", \"×\": \"x\", \"²\": \"2\", \"—\": \"-\", \"–\": \"-\", \"’\": \"'\", \"_\": \"-\", \"`\": \"'\", '“': '\"', '”': '\"', '“': '\"', \"£\": \"e\", '∞': 'infinity', 'θ': 'theta', '÷': '/', 'α': 'alpha', '•': '.', 'à': 'a', '−': '-', 'β': 'beta', '∅': '', '³': '3', 'π': 'pi'}\n",
    "#spell_mapping = {'colour': 'color', 'centre': 'center', 'favourite': 'favorite', 'travelling': 'traveling', 'counselling': 'counseling', 'theatre': 'theater', 'cancelled': 'canceled', 'labour': 'labor', 'organisation': 'organization', 'wwii': 'world war 2', 'citicise': 'criticize', 'youtu ': 'youtube ', 'Qoura': 'Quora', 'sallary': 'salary', 'Whta': 'What', 'narcisist': 'narcissist', 'howdo': 'how do', 'whatare': 'what are', 'howcan': 'how can', 'howmuch': 'how much', 'howmany': 'how many', 'whydo': 'why do', 'doI': 'do I', 'theBest': 'the best', 'howdoes': 'how does', 'mastrubation': 'masturbation', 'mastrubate': 'masturbate', \"mastrubating\": 'masturbating', 'pennis': 'penis', 'Etherium': 'Ethereum', 'narcissit': 'narcissist', 'bigdata': 'big data', '2k17': '2017', '2k18': '2018', 'qouta': 'quota', 'exboyfriend': 'ex boyfriend', 'airhostess': 'air hostess', \"whst\": 'what', 'watsapp': 'whatsapp', 'demonitisation': 'demonetization', 'demonitization': 'demonetization', 'demonetisation': 'demonetization', 'pokémon': 'pokemon'}\n",
    "# Full dicitonary that will be corrected\n",
    "#correction_mapping = {**contraction_mapping,**punct_mapping,**spell_mapping}\n",
    "# Define function that will take the contractions and replace them with the non-contracted phrase\n",
    "# Note currently does not replace contractions next to punctuation...\n",
    "# Commented code below does, but takes a few min vs. a few sec.\n",
    "#def clean_questions(text):\n",
    "#    specials = [\"’\", \"‘\", \"´\", \"`\"]\n",
    "#    for s in specials:\n",
    "#        text = text.replace(s, \"'\")\n",
    "#    text = ' '.join([correction_mapping[t] if t in correction_mapping else t for t in text.lower().split(\" \")])\n",
    "#    return text\n",
    "# Now apply the function to the data\n",
    "# Note we include the unknown data here too, as it must be converted for the model\n",
    "#trainingdf_cleaned = trainingdf.iloc[:,1].apply(lambda x: clean_questions(x))\n",
    "#unknowndf_cleaned = unknowndf.iloc[:,1].apply(lambda x: clean_questions(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "e2352bc10a69c6c1e4c9245777a5e117a427d800",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#%%time\n",
    "# Dictionaries of various corrections\n",
    "contraction_mapping = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\", \"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\", \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",  \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\", \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\", \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\", \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\", \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\", \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\", \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\", \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\", \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\", \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\", \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",  \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\", \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\", \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\", \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\"}\n",
    "punct_mapping = {\"‘\": \"'\", \"₹\": \"e\", \"´\": \"'\", \"°\": \"\", \"€\": \"e\", \"™\": \"tm\", \"√\": \" sqrt \", \"×\": \"x\", \"²\": \"2\", \"—\": \"-\", \"–\": \"-\", \"’\": \"'\", \"_\": \"-\", \"`\": \"'\", '“': '\"', '”': '\"', '“': '\"', \"£\": \"e\", '∞': 'infinity', 'θ': 'theta', '÷': '/', 'α': 'alpha', '•': '.', 'à': 'a', '−': '-', 'β': 'beta', '∅': '', '³': '3', 'π': 'pi'}\n",
    "spell_mapping = {'colour': 'color', 'centre': 'center', 'favourite': 'favorite', 'travelling': 'traveling', 'counselling': 'counseling', 'theatre': 'theater', 'cancelled': 'canceled', 'labour': 'labor', 'organisation': 'organization', 'wwii': 'world war 2', 'citicise': 'criticize', 'youtu ': 'youtube ', 'Qoura': 'Quora', 'sallary': 'salary', 'Whta': 'What', 'narcisist': 'narcissist', 'howdo': 'how do', 'whatare': 'what are', 'howcan': 'how can', 'howmuch': 'how much', 'howmany': 'how many', 'whydo': 'why do', 'doI': 'do I', 'theBest': 'the best', 'howdoes': 'how does', 'mastrubation': 'masturbation', 'mastrubate': 'masturbate', \"mastrubating\": 'masturbating', 'pennis': 'penis', 'Etherium': 'Ethereum', 'narcissit': 'narcissist', 'bigdata': 'big data', '2k17': '2017', '2k18': '2018', 'qouta': 'quota', 'exboyfriend': 'ex boyfriend', 'airhostess': 'air hostess', \"whst\": 'what', 'watsapp': 'whatsapp', 'demonitisation': 'demonetization', 'demonitization': 'demonetization', 'demonetisation': 'demonetization', 'pokémon': 'pokemon'}\n",
    "# Full dicitonary that will be corrected\n",
    "correction_mapping = {**contraction_mapping,**punct_mapping,**spell_mapping}\n",
    "# Define function that will take the contractions and replace them with the non-contracted phrase\n",
    "def correct_questions(text):\n",
    "    specials = [\"’\", \"‘\", \"´\", \"`\"]\n",
    "    for s in specials:\n",
    "        text = text.replace(s, \"'\")\n",
    "    for contraction, replacement in correction_mapping.items():\n",
    "        text = re.sub(contraction, replacement, text.lower())\n",
    "    return text\n",
    "# Now apply the function to the data\n",
    "# Note we include the unknown data here too, as it must be converted for the model\n",
    "trainingdf_corrected = trainingdf.iloc[:,1].apply(lambda x: correct_questions(x))\n",
    "unknowndf_corrected = unknowndf.iloc[:,1].apply(lambda x: correct_questions(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "99521f7da27d729d1494a2ca2a6e78ddace55043",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#%%time\n",
    "# We can also try to take care of possessives, many of which are not in the embedding\n",
    "# Note we do this after we take care of contractions to avoid removing it's and other similar contractions\n",
    "trainingdf_cleaned = trainingdf_corrected.apply(lambda x: re.sub(\"'s\", \"\", x))\n",
    "unknowndf_cleaned = unknowndf_corrected.apply(lambda x: re.sub(\"'s\", \"\", x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "0e210a7bb623fc0490da53d0d8079f3cb138aca0",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set up the tokenizer first\n",
    "# This will split the words and lower them\n",
    "tokenizer = Tokenizer()\n",
    "# Fit the tokenizer to the comments, so it can build the integer assignments\n",
    "tokenizer.fit_on_texts(pd.concat([trainingdf_cleaned,unknowndf_cleaned]))\n",
    "# Use the word-integer assignments to covert the comments into a list of integers\n",
    "sequences = tokenizer.texts_to_sequences(trainingdf_cleaned)\n",
    "unknownsequences = tokenizer.texts_to_sequences(unknowndf_cleaned)\n",
    "\n",
    "# Save the dictionary between words and integers for later use\n",
    "word_index = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "38e6fe24d3b0e4fc58d609b20779f41d6fa348a5",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# In order to put the comments (now integer lists) through the neural network, they need to be the same length.\n",
    "# So we find the longest comment, and then pad the rest of the shorter comments with zeros.\n",
    "# Note this does involve having to consider the \n",
    "maxsequncelength = max([len(max(sequences, key=len)),len(max(unknownsequences, key=len))])\n",
    "data = pad_sequences(sequences, maxlen=maxsequncelength)\n",
    "unknowndata = pad_sequences(unknownsequences, maxlen=maxsequncelength)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "1cd02ec9d98e509f3dcd882b5a013a89051f9d4b",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Split data into testing and training sets. Can assign split ratio, though it is left as the default here.\n",
    "# Though our test data will be our validation data here, as the true test data would be submitted to the competition.\n",
    "testtrainsplit = 0.20;\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, trainingdf.iloc[:,2:], test_size = testtrainsplit, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "7d12bfcf15a84e31cda4216dcc11d7203d4738d9",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This will resample the data to include more insincere questions. However, it doesn't seem to help much.\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "X_res, y_res = RandomOverSampler(sampling_strategy=0.2).fit_resample(X_train, y_train.values.ravel())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "502007da0e1798a76254a3668d4978e69bb14792"
   },
   "source": [
    "## Embedding\n",
    "\n",
    "In order to feed this to a neural network, we want to do some dimensional reduction/normalization of the data. The comments so far have been turned into a vector of integers. But these values are not normalized, which they need to be for the neural network.\n",
    "\n",
    "The standard apporach for machine learning would be to one-hot encode each work instead of a single integer, leading to the comemnts be represented by an array. However, this would result in a very large dimensional, and sparse, set of vectors/arrays. This is undesirable for the neural network.\n",
    "\n",
    "Instead it is better to encode the words in a word vectors that care more about co-occurence. To start, we will be using a pre-trained word vector representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "d0f757d90b58911c214872ed3dcba1072ab9cca6"
   },
   "outputs": [],
   "source": [
    "# Here we intialize our embedding dictionary\n",
    "embeddings_index = {}\n",
    "# There are a number of different dimensional word-representations\n",
    "# However, for this project we are limited in which word-vectors to choose\n",
    "embeddingdim = 300;\n",
    "# Below we just read the file and store it in the initialized dictionary\n",
    "f = open('../input/embeddings/wiki-news-300d-1M/wiki-news-300d-1M.vec',encoding='UTF-8')\n",
    "for line in f:\n",
    "    values = line.split(' ')\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "1ee6a7ed86828146c7b6be7e260fa26f5cc1fac7"
   },
   "source": [
    "With the word vectors in hand, we set up the translation between our tokenized word integer and their corresponding word vector representation as a matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "8ef97c1a488a6bae3ae047a5a6c53ce0f5b93798",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# start with the an array of zeros. Note the shape is (num. of words, dim. of GloVe)\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, embeddingdim))\n",
    "unknown_words = []\n",
    "# Go through our word/integer dictionary\n",
    "for word, i in word_index.items():\n",
    "    # For each word, find the corresponding rep in GloVe and set embedding vector\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "    else:\n",
    "        unknown_words.append(word)\n",
    "    # Note if the embedding_vector is not found, remains a vector of zeros as initialized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "5bd25ef729f9d47228165d6955f9bc38f4528b32"
   },
   "source": [
    "With the embedding matrix found, we simply set up the embedding layer with this matrix as the first layer of our neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "b6eb3c2880362640d7d5a993e90542a37846f8bc",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embedding_layer = Embedding(len(word_index) + 1,\n",
    "                            embeddingdim,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=maxsequncelength,\n",
    "                            trainable=False)\n",
    "# Note the first two arguments are the shape of the weights, \n",
    "# but the expected input length is still the length of the longest comment\n",
    "# Also we have set trainable=False, as we do not want the weights to change.\n",
    "# Finally, note the embedding_matrix has knowledge of the unknown (true test) date\n",
    "# but since this is not a trainable layer, it shouldn't be a problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "59d8906703df67edb7e0afa17c2cc2e0c537ea73"
   },
   "source": [
    "## Hybrid Neural Network\n",
    "\n",
    "Now we build our network. This will begin with our embedding layer, followed by one convolutional layer with relu activations. This will feed into a LSTM layer, which will then feed into a dense layer, also with relu activation, before heading to the final layer with sigmoid activation (as opposed to softmax, as these labels are independant). \n",
    "\n",
    "The model will use the binary crossentropy as the loss function, as the labels should be independant, and will be scored with the accuracy metric. The optimizer is a simple Adam optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "8cf5fc6dc7e481898bea2ef737365ea54cb42e38"
   },
   "outputs": [],
   "source": [
    "# Now we will define a function to setup the model\n",
    "def model_setup(rfs_var, mps_var, actc_var, actd_var, met_var, drpout_var, node_num_var):\n",
    "    # We will begin with a simple sequential model\n",
    "    model = Sequential()\n",
    "    # Frist layer is the embedding layer as we had defined above\n",
    "    model.add(embedding_layer)\n",
    "    # We enxt have one convolutional layer, with relu activation and pooling layer\n",
    "    model.add(Conv1D(node_num_var, rfs_var, activation=actc_var))\n",
    "    model.add(MaxPooling1D(mps_var))\n",
    "    # Next we add a LSTM layer\n",
    "    model.add(LSTM(node_num_var))\n",
    "    model.add(Dropout(drpout_var))\n",
    "    # Next we flatten and send into the dense layer\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(node_num_var, activation=actd_var))\n",
    "    # Finally we forward the results from the dense layer into the final prediction layer.\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    # We then compile the model with the layers, using cross-entropy \n",
    "    model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=[met_var])\n",
    "    # We can then fit the model, using our test data as validation\n",
    "    model.fit(X_res, y_res, validation_data=(X_test, y_test),\n",
    "          epochs=1, batch_size=node_num_var)\n",
    "    \n",
    "    return model\n",
    "    \n",
    "# Model hyperparameters, these are chosen by some optimization below\n",
    "# Size of the receptive field for convolutional layers\n",
    "rfs = 5\n",
    "# Size of the max pooling windows\n",
    "mps = 2\n",
    "# Activation for convolutional layer\n",
    "actc = 'relu'\n",
    "# Activation for dense layer\n",
    "actd = 'relu'\n",
    "# Metric for the eval\n",
    "met = 'acc'\n",
    "# The dropout ratio for the dropout layers\n",
    "drpout = 0.1\n",
    "# The number of nodes for the layers\n",
    "node_num = 256\n",
    "\n",
    "# Now we run the model\n",
    "model = model_setup(rfs, mps, actc, actd, met, drpout, node_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "492a7c32a365458df6461c2deb8046ca1f42cf68"
   },
   "source": [
    "Nowe we turn to computing the F1 score. To do this, we need to find the optimal threshold for decision. We can use the scipy function minimize to optimize this. Since we actually want a maximium, the signs need to be fliped but otherwise it is very straightforward. \n",
    "\n",
    "We will do this maximization on the training data and use the threshold from that on the test/validation data. This will allow us to examine possible under/overfitting of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "7a6a20efeeb1699eff18663c2245c99aba7ab4ca"
   },
   "outputs": [],
   "source": [
    "def f1_score_computation():\n",
    "    # In order to compute the F1 score, we need to apply the model to the data\n",
    "    CNN_probas_train = model.predict(X_train)\n",
    "    CNN_probas_test = model.predict(X_test)\n",
    "\n",
    "    # Start by defining the function to be optimized. Note the -ve!\n",
    "    def f1_score_function(x):\n",
    "        return -f1_score(y_train, (CNN_probas_train > x).astype(np.int))\n",
    "    # Do mutliple minimizations to find global min instead of local\n",
    "    # Save as dict w/ threshold:score, making sure to flip sign for true score\n",
    "    results = {}\n",
    "    for i in range(1,10):\n",
    "        results[minimize(f1_score_function, i*0.1, method='nelder-mead').x[0]] = -minimize(f1_score_function, i*0.1, method='nelder-mead').fun\n",
    "    # Extract the threshold and F1 score of best threshold\n",
    "    threshold = max(results.items(), key=lambda x: x[1])[0]\n",
    "    max_f1score = max(results.items(), key=lambda x: x[1])[1]\n",
    "    \n",
    "    return (threshold, max_f1score, CNN_probas_test)\n",
    "\n",
    "f1_results = f1_score_computation()\n",
    "# Print the results\n",
    "print(\"Decision threshold: \"+str(f1_results[0]))\n",
    "print(\"Max training F1 score: \"+str(f1_results[1]))\n",
    "\n",
    "# To check for overfitting, we can look at the F1 score of the training set\n",
    "print(\"Testing F1 score: \"+str(f1_score(y_test, (f1_results[2] > f1_results[0]).astype(np.int))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "441677b7bd9b4d35d0828ddcc0378031256f88c5"
   },
   "source": [
    "Note that these scores are not great compared to the competition scores, but there could be a lot more done to improve this model. This includes more data cleaning before tokenization, tuning hyperparameters in the network, treating the class imbalance differently, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "de08494e55fd50e1ecb87062917f4ebcf502940c"
   },
   "source": [
    "## Examining Results and Debugging\n",
    "\n",
    "Below is largely troubleshooting and more specific details of the results from the machine learning alogrithm. This can all be commented out for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a44c5be2cccce45136314e6666e3025504e343b1",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# If we want, we can have a summary of the weights and shapes of the layers\n",
    "#model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "822953fcd0b7404fdb0423fd4a7e16eaed3f248e",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We will also look at the results to check reasonableness.\n",
    "# e.g. we are not picking all the same prediction, they are not too small or large, etc\n",
    "#CNN_probas_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "b5984ac5a60b3dbbe7910bc5887ed2a185b3179c",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We can got through a number of different hyperparameters here\n",
    "# This is done by hand, but likely that there is a better method\n",
    "\n",
    "# Size of the receptive field for convolutional layers\n",
    "#trial_rfs = [3, 4, 5]\n",
    "# Size of the max pooling windows\n",
    "#trial_mps = [2, 3]\n",
    "# Activation for convolutional layer\n",
    "#trial_actc = ['tanh', 'relu']\n",
    "# Activation for dense layer\n",
    "#trial_actd = ['relu']\n",
    "# Metric for the eval\n",
    "#trial_met = ['acc']\n",
    "# The dropout ratio for the dropout layers\n",
    "#trial_drpout = [0.1, 0.3, 0.5, 0.7, 0.9]\n",
    "# The number of nodes for the layers\n",
    "#trial_node_num = [128, 256]\n",
    "\n",
    "#import itertools\n",
    "#results = {}\n",
    "#for a, b, c, d, e, f, g in itertools.product(trial_rfs, trial_mps, trial_actc, trial_actd, trial_met, trial_drpout, trial_node_num):\n",
    "#    model = model_setup(a, b, c, d, e, f, g)\n",
    "#    f1_results = f1_score_computation()\n",
    "#    results[f1_score(y_test, (f1_results[2] > f1_results[0]).astype(np.int))] = [a,b,c,d,e,f,g]\n",
    "    \n",
    "#max_f1_score_trials = max(results.items(), key=lambda x: x[0])[0]\n",
    "#max_f1_vars_trials = max(results.items(), key=lambda x: x[0])[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a5623b7cf2bfba85c8399fca88a6195cdbd6a7bc",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#(max_f1_score_trials, max_f1_vars_trials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "d2e9f64bc0b910c17c3d700075b155c0c3dcb59b",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "2535677a72f51ad3cc62e6f6cbcabdbeeee08285",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Now we want to which ones of the validation set was mislabeled\n",
    "#Start by comparing the test labels with the predicitons\n",
    "#test_labels_df = y_test.reset_index().drop(['index'],axis=1)\n",
    "#predictions_df = pd.DataFrame((CNN_probas_test > threshold).astype(np.int).flatten(),columns=['target'])\n",
    "#match_df = (test_labels_df==predictions_df).rename_axis({'target':'match'},axis=1)\n",
    "#comparison_df = pd.concat([y_test.reset_index()['index'], match_df,test_labels_df],axis=1)\n",
    "\n",
    "# Extract the confusion matrix\n",
    "#true_positive = comparison_df[(comparison_df['match']==True) & (comparison_df['target']==1)]\n",
    "#true_negative = comparison_df[(comparison_df['match']==True) & (comparison_df['target']==0)]\n",
    "#false_negative = comparison_df[(comparison_df['match']==False) & (comparison_df['target']==1)]\n",
    "#false_positive = comparison_df[(comparison_df['match']==False) & (comparison_df['target']==0)]\n",
    "\n",
    "#confusion_matrix = pd.DataFrame([['TN: '+str(len(true_negative)),'FN: '+str(len(false_negative))],['FP: '+str(len(false_positive)),'TP: '+str(len(true_positive))]],columns = ['Sincere','Insincere'], index = ['Pred. Sincere','Pred. Insincere'])\n",
    "#print('Sincere: '+str(len(test_labels_df[test_labels_df['target']==0])))\n",
    "#print('Insincere: '+str(len(test_labels_df[test_labels_df['target']==1])))\n",
    "#confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "5a2f339a654b7b1a8bc8becac4ae273b7ed196fe"
   },
   "source": [
    "## Applying the model\n",
    "\n",
    "Next we can apply the trained model to the test data. We will use the threshold calculated above to compute give the predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "f2c9ca0860561c6708e93c3e31b6bcc2bef627b2"
   },
   "outputs": [],
   "source": [
    "submission = pd.DataFrame(\n",
    "    {'qid':unknowndf['qid'], 'prediction':(model.predict(unknowndata) > f1_results[0]).astype(np.int).flatten()},\n",
    "    columns = ['qid','prediction'])\n",
    "submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "61f28bf608065237d003afc983973a7a0ef263b4",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "submission.to_csv('submission.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The kaggle score for this submission was 0.65369, which was within the top 31% of submissions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
